<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Hi Folks</title><link>https://huanggze.top/</link><description>Recent content on Hi Folks</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>© huanggze 2021</copyright><lastBuildDate>Sat, 17 Sep 2022 16:18:32 +0800</lastBuildDate><atom:link href="https://huanggze.top/index.xml" rel="self" type="application/rss+xml"/><item><title>从源码中学习优雅 Go 编程</title><link>https://huanggze.top/posts/learn-from-source-code/</link><pubDate>Sat, 17 Sep 2022 16:18:32 +0800</pubDate><guid>https://huanggze.top/posts/learn-from-source-code/</guid><description>1. 巧妙使用 Context 包的 Value 函数 Go net 包源码中有一段如下代码：
1 2 3 4 5 6 7 8 9 10 11 12 13 func (r *Resolver) lookupIPAddr(ctx context.Context, network, host string) ([]IPAddr, error) { // ... // The underlying resolver func is lookupIP by default but it // can be overridden by tests. This is needed by net/http, so it // uses a context key instead of unexported variables. resolverFunc := r.</description></item><item><title>GitLab CI 工作原理</title><link>https://huanggze.top/posts/dive-into-gitlab-ci/</link><pubDate>Wed, 24 Aug 2022 22:24:19 +0800</pubDate><guid>https://huanggze.top/posts/dive-into-gitlab-ci/</guid><description>本文介绍，当我们向 GitLab 代码仓库提交代码后（merge request），GitLab Server 如何触发流水线和执行 CI Job。了解 GitLab CI 工作原理可以帮助我们更好实践和优化 CI 流程，提高开发效率。
概念 GitLab CI 功能涉及以下组件：
GitLab Server：GitLab 后端，通过 REST API 提供各种能力，包括 Git 仓库管理、CI/CD 功能； GitLab Runner：和 Jenkins 的 agent 是一个概念，负责从 GitLab Server 中不断地拿到待执行 Job，并启动一个 Executor 去执行。Runner 可以是二进制部署，也可以是部署为一个 Docker 容器，或者是 K8s 集群中的 Pod； GitLab Executor：CI Job 执行器，实际处理 CI 任务。GitLab 支持多种 Executor，比如：一个 CI Job 可以在一个本地物理机上执行、或一个每次新建的虚拟机环境中执行、或 Docker 容器以及 K8s Pod（使用时新建，结束时销毁）。 比如，我的开发环境选型是二进制部署 Runner，每台机器上各部署一个，执行器 Executor 采用 Docker。
GitLab CI 架构 GitLab 官方文档提供了 GitLab CI 架构图如下1：</description></item><item><title>Go 环境变量</title><link>https://huanggze.top/posts/go-environment-variables/</link><pubDate>Sat, 21 May 2022 11:37:29 +0800</pubDate><guid>https://huanggze.top/posts/go-environment-variables/</guid><description>go 的 runtime 包提供一些控制 Go 程序运行时的操作，其中包括 go 环境变量1。
GOTRACEBACK GOTRACEBACK 环境变量用于控制当 Go 程序 panic 时输出栈信息的多少。变量值可以为：
none：不输出任何信息 single：仅输出当前崩溃的 goroutine 信息（默认值） all：打印所有由用户创建的 goroutine 信息 system：类似 all，并包含系统创建的 goroutine crash：类似 system，但不是直接 panic 退出，而是执行操作系统的一些操作。比如，在 Unix 系统上会生成 core dump 修改 GOTRACEBACK 有两种方式，通过 runtime/debug 包在代码里修改：
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 package main import ( &amp;#34;runtime/debug&amp;#34; &amp;#34;time&amp;#34; ) func main() { debug.</description></item><item><title>go 命令行工具</title><link>https://huanggze.top/posts/go-commands/</link><pubDate>Tue, 17 May 2022 23:35:55 +0800</pubDate><guid>https://huanggze.top/posts/go-commands/</guid><description>go build go build 用于编译代码，生成可执行文件。如执行 go build ed.go 会生成 ed 或 ed.exe12。
参数说明 -gcflags 编译器参数。一组传递给 go tool compile 的参数，可通过查 go tool compile 命令查到有哪些支持的参数3，如 -N 关闭编译器优化，-l 关闭内联（inlining）：
1 2 3 go build -gcflags &amp;#34;-N -l&amp;#34; main.go go build -gcflags=&amp;#34;-N -l&amp;#34; main.go go build -gcflags=-N -gcflags=-l main.go -ldflags 链接器参数。一组传递给 go tool link 的参数4，如 -X importpath.name=value 设置包内指定变量的值，该变量可以是未赋值或初始化了的：
1 2 3 4 5 6 7 package main var version = &amp;#34;1.0&amp;#34; func main() { println(version) } 1 2 3 $ go build -ldflags=&amp;#34;-X main.</description></item><item><title>Git 学习</title><link>https://huanggze.top/posts/learn-git/</link><pubDate>Mon, 16 May 2022 21:46:44 +0800</pubDate><guid>https://huanggze.top/posts/learn-git/</guid><description>git rev-list 按时间倒序列出 commit（即 reverse-list）。
假设 commit 提交历史从近到远有 5 条：
1 2 3 4 5 6 $ git log E - (HEAD -&amp;gt; master) addE (10/19/2019 13:31:19) D - addD (10/18/2019 13:31:19) C - addC (10/17/2019 13:31:19) B - addB (10/16/2019 13:31:19) A - addA (10/15/2019 13:31:19) 例子1：
git rev-list D：打印 D 及更早的 commit 节点
git rev-list D...B：打印 D 到 B 之间的节点，且不包括 B
git rev-list D ^B：打印 D 及更早的节点，并剔除 B 及更早的节点（同上）</description></item><item><title>Makefile 语法学习</title><link>https://huanggze.top/posts/learn-makefile/</link><pubDate>Sun, 15 May 2022 21:10:16 +0800</pubDate><guid>https://huanggze.top/posts/learn-makefile/</guid><description>变量赋值 简单赋值（:=） 一旦 make 读入该变量的定义语句，赋值运算符右边部分会立刻扩展，而扩展后的文本会被存储成该变量的值1：
1 2 3 x := foo y := $(x) bar x := later 等同于：
1 2 x := later y := foo bar 递归变量（=） 扩展的动作会被延迟到该变量被使用的时候进行。
1 2 3 x = foo y = $(x) bar x = later 条件变量（?=） 只会在变量的值尚不存在是进行赋值操作。
条件语句 ifeq：是否相等
1 2 3 ifeq (arg1, arg2) ... endif ifneq：是否不等
1 2 3 4 5 ifneq (arg1, arg2) ... else ... endif ifdef：是否有值
1 2 3 ifdef variable-name .</description></item><item><title>GitLab CI/CD</title><link>https://huanggze.top/posts/gitlab-cicd/</link><pubDate>Mon, 09 May 2022 22:59:10 +0800</pubDate><guid>https://huanggze.top/posts/gitlab-cicd/</guid><description>概念 术语 说明 pipeline 流水线 stage 阶段 job 任务 .gitlab-ci.yml CI/CD配置文件 GitLab Runner 负责运行job的Agent。使用前需要先安装注册 Executor 注册Runner时，需要选择一个Executor作为运行环境，如 Docker、K8s1 2 预置变量 CI/CD预置变量，每个流水线都有这些变量 Pipeline 类型 Basic pipelines：基本流水线，同一 Stage 内并发执行，不同 Stage 顺序执行。 DAG pipelines：有向无环流水线，使用 needs 指明 Job 间的依赖关系，优化并发流程。 Merge request pipelines：又称 branch pipeline，提交 MR 的时候触发，需要配合 only 或 rules 使用。 1 2 3 4 5 6 7 8 9 10 11 12 default: image: ubuntu:latest build-job: only: - merge_requests script: - echo &amp;#39;build-job&amp;#39; test-job: script: - echo &amp;#39;test-job&amp;#39; 如图，提交的 MR 只触发了 build-job：</description></item><item><title>Linux 命令：ulimit</title><link>https://huanggze.top/posts/linux-cmd-ulimit/</link><pubDate>Fri, 08 Apr 2022 21:41:46 +0800</pubDate><guid>https://huanggze.top/posts/linux-cmd-ulimit/</guid><description>ulimit 可以用来控制 shell 执行程序资源使用1。
查看全部资源限制 1 2 3 4 5 6 7 8 9 10 $ ulimit -a -t: cpu time (seconds) unlimited -f: file size (blocks) unlimited -d: data seg size (kbytes) unlimited -s: stack size (kbytes) 8192 -c: core file size (blocks) 0 -v: address space (kbytes) unlimited -l: locked-in-memory size (kbytes) unlimited -u: processes 2784 -n: file descriptors 256 查看最多占用 CPU 的时间 1 2 $ ulimit -t unlimited 设置 CPU 使用时间上限 1 $ ulimit -t 1 使用 CPU 密集型计算任务测试：</description></item><item><title>Go 日志：zap</title><link>https://huanggze.top/posts/go-logging-part2/</link><pubDate>Thu, 07 Apr 2022 16:19:16 +0800</pubDate><guid>https://huanggze.top/posts/go-logging-part2/</guid><description>zap 是 Uber 开源的 logging 库。
Zap Logger Zap 提供两种日志记录器（logger）：Sugared Logger 和 Logger。前者者支持结构化日志以及 Printf 风格日志，后者只支持结构化日志，但后者性能更好。
1 2 3 4 5 6 // {&amp;#34;level&amp;#34;:&amp;#34;info&amp;#34;,&amp;#34;ts&amp;#34;:1649341600.931539,&amp;#34;caller&amp;#34;:&amp;#34;awesomeProject/main.go:10&amp;#34;,&amp;#34;msg&amp;#34;:&amp;#34;hello&amp;#34;,&amp;#34;val1&amp;#34;:1,&amp;#34;val2&amp;#34;:&amp;#34;two&amp;#34;} logger, _ := zap.NewProduction() logger.Info(&amp;#34;hello&amp;#34;, zap.Int(&amp;#34;val1&amp;#34;, 1), zap.String(&amp;#34;val2&amp;#34;, &amp;#34;two&amp;#34;), ) 1 2 3 4 5 6 logger, _ := zap.NewProduction() sugar := logger.Sugar() // {&amp;#34;level&amp;#34;:&amp;#34;info&amp;#34;,&amp;#34;ts&amp;#34;:1649341600.931539,&amp;#34;caller&amp;#34;:&amp;#34;awesomeProject/main.go:10&amp;#34;,&amp;#34;msg&amp;#34;:&amp;#34;hello&amp;#34;,&amp;#34;val1&amp;#34;:1,&amp;#34;val2&amp;#34;:&amp;#34;two&amp;#34;} sugar.Infow(&amp;#34;hello&amp;#34;, zap.Int(&amp;#34;val1&amp;#34;, 1), zap.String(&amp;#34;val2&amp;#34;, &amp;#34;two&amp;#34;)) // {&amp;#34;level&amp;#34;:&amp;#34;info&amp;#34;,&amp;#34;ts&amp;#34;:1649342764.54033,&amp;#34;caller&amp;#34;:&amp;#34;awesomeProject/main.go:12&amp;#34;,&amp;#34;msg&amp;#34;:&amp;#34;val1=1, val2=two&amp;#34;} sugar.Infof(&amp;#34;val1=%d, val2=%s&amp;#34;, 1, &amp;#34;two&amp;#34;) Zap 支持三种方式创建预置 logger：NewExample()、NewProduction()、NewDevelopment()。它们的区别是日志信息内容。
定制 Logger 你也可以定制 logger（custom logger）来实现额外功能，比如输出日志到文件1。此时，需要使用 New() 方法，而不是前面的 NewXXX()。New() 方法的函数签名如下。它需要接收一个 zapcore 参数。</description></item><item><title>Go 日志：klog</title><link>https://huanggze.top/posts/go-logging-part1/</link><pubDate>Wed, 06 Apr 2022 21:23:09 +0800</pubDate><guid>https://huanggze.top/posts/go-logging-part1/</guid><description>klog 是 K8s 社区维护的 logging 库，支持在程序命令行注册以下 flag1：
log_file：输出到日志文件； log_file_max_size：日志文件最大大小（单位：mb），如果超过最大值则会擦除日志文件全部内容，并从头开始写日志。未设置最大值时，无限制； logtostderr：是否输出到标准错误输出，如果想输出到文件，改值应设为 false； skip_log_headers：忽略记录日志元信息； v：日志等级 示例 1：手动/自动输出日志到文件 1 2 3 4 5 6 7 8 9 10 11 12 13 14 func main() { fs := flag.NewFlagSet(&amp;#34;klog&amp;#34;, flag.ExitOnError) // 注册 flag klog.InitFlags(fs) fs.Set(&amp;#34;skip_log_headers&amp;#34;, &amp;#34;true&amp;#34;) // 解析 flag fs.Parse(os.Args[1:]) klog.Info(&amp;#34;nice to meet you&amp;#34;) klog.ErrorS(errors.New(&amp;#34;oops&amp;#34;), &amp;#34;noooo&amp;#34;) // 手动刷新日志记录到文件 klog.Flush() // 或者每 5s 自动 flush // time.Sleep(6 * time.Second) } 1 2 3 4 $ go run main.</description></item><item><title>Linux 命令：nohup</title><link>https://huanggze.top/posts/linux-cmd-nohup/</link><pubDate>Wed, 06 Apr 2022 11:42:49 +0800</pubDate><guid>https://huanggze.top/posts/linux-cmd-nohup/</guid><description>nohup，no hang up（不挂起），用于在系统后台不挂断地运行命令，即使退出终端不会影响程序的运行1。
nohup 命令，在默认情况下（非重定向时），会输出一个名叫 nohup.out 的文件到当前目录下，如果当前目录的 nohup.out 文件不可写，输出重定向到 $HOME/nohup.out 文件中。
如果要停止运行，需要使 ps 命令找到 nohup 运行脚本的 PID，然后使用 kill 命令来删除。
Linux nohup 命令&amp;#160;&amp;#x21a9;&amp;#xfe0e;</description></item><item><title>Linux 命令：ss</title><link>https://huanggze.top/posts/linux-cmd-ss/</link><pubDate>Wed, 02 Mar 2022 14:09:57 +0800</pubDate><guid>https://huanggze.top/posts/linux-cmd-ss/</guid><description>ss（Socket Statistics）1命令功能类似于 netstat，用于获取 socket 统计信息，比 netstat 能展示更多信息。
命令格式 ss [options] [ FILTER ]
选项 参数 描述 -H, --no-header 不打印头部标题行 -n, --numeric 不解析域名 -r, --resolve 解析域名 -a, --all 显示正在监听和非监听（对于 TCP 来说是已建立的连接）的 socket -l, --listening 显示正在监听的 socket（默认不显示） -p, --processes socket 对应的进程 -4, --ipv4 只显示 ipv4 socket -t, --tcp 只显示 TCP socket -u, --udp 只显示 UDP socket -x, --unix 只显示 Unix 域套接字（Unix domain sockets） 过滤器 按 socket 状态过滤 1 2 ss state [ STATE-FILTER ] [ EXPRESSION ] ss exclude [ STATE-FILTER ] [ EXPRESSION ] TCP 状态有 LISTEN、SYN-SENT、SYN-RECEIVED、SYN-RECEIVED、ESTABLISHED、FIN-WAIT-1、FIN-WAIT-2、CLOSE-WAIT、CLOSING、LAST-ACK、 TIME-WAIT、CLOSED2。STATE-FILTER 对应可选项有 all、connected、synchronized、bucket、big。</description></item><item><title>Go pprof 性能分析</title><link>https://huanggze.top/posts/intro-pprof/</link><pubDate>Mon, 28 Feb 2022 16:26:32 +0800</pubDate><guid>https://huanggze.top/posts/intro-pprof/</guid><description>PProf Golang 提供了程序性能分析工具 pprof。性能分析（profiling）可以收集运行期间的程序性能情况，弥补了其他静态测试方法的不足。pprof 支持三种方式采样和生成性能报告（profile）：
通过 runtime/pprof 包将采集结果保存到文件； 通过 net/http/pprof 打开 /debug/pprof HTTP 接口暴露数据； go test -cpuprofile=cpu.profile 命令采集测试用例运行的性能数据。 1 2 3 4 5 6 7 8 9 10 11 12 // 分析报告导出为文件 func main() { f, _ := os.Create(&amp;#34;cpu.profile&amp;#34;) defer f.Close() pprof.StartCPUProfile(f) defer pprof.StopCPUProfile() for i := 0; i &amp;lt; 3; i++ { go sum() } } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 func sum() { d := 0 for i := 0; i &amp;lt; 10_000_000_000; i++ { d += i } } func main() { for i := 0; i &amp;lt; 3; i++ { go sum() } // 使用 go tool pprof -web 127.</description></item><item><title>开源数据约束语言 CUE</title><link>https://huanggze.top/posts/intro-cue/</link><pubDate>Mon, 28 Feb 2022 09:42:28 +0800</pubDate><guid>https://huanggze.top/posts/intro-cue/</guid><description>CUE（Configure Unify Execute）是一种数据验证语言，有自己的推理引擎，可作为配置语言使用。CUE 语言的特点是把数据类型（type）和值（value）看作同一概念：Types are Values。如下图展示了 CUE 的核心思想，左边是数据，中间是类型，右边（.cue）是数据与类型的混合，约束了任何 largeCapital 数据必须包含字符串类型的 name 字段，大于 5M 的 pop 字段以及值为 true 的 capital 字段。
CUE 的主要功能包含：
数据验证：客户侧数据校验 代码导入/导出：支持从 Go 代码、Protobuf、YAML、JSON 等源文件转换成 CUE 文件 配置管理：是 JSON 的超集，支持类型检查 辅助工具：cue trim 自动修剪冗余 CUE 示例 下面 cue 文件定义了三个字段，a 是 int 类型，值为 1；b 是对象类型；c 是 string 类型，未赋具体指。
1 2 3 4 5 6 7 // demo.cue a: int a: 1 b: { c: &amp;#34;abc&amp;#34; } d: string 执行验证命令 cue eval demo.</description></item><item><title>Ginkgo 测试框架介绍</title><link>https://huanggze.top/posts/intro-ginkgo/</link><pubDate>Fri, 25 Feb 2022 21:32:07 +0800</pubDate><guid>https://huanggze.top/posts/intro-ginkgo/</guid><description>除了 Go testing 包提供的测试框架，还可以使用 Ginkgo 测试框架。Ginkgo 是一个行为驱动开发（Behavior Driven Development，BDD）测试框架。BDD 是一种敏捷开发技术，建立在测试驱动开发（Test Driven Development，TDD）基础之上，强调使用 DSL（Domain Specific Language，领域特定语言）描述用户行为、定义业务需求，是需求分析人员、开发人员与测试人员进行沟通的有效方法1。行为驱动开发的核心在于&amp;quot;行为&amp;quot;。当业务需求被划分为不同的业务场景，并以 &amp;ldquo;Given-When-Then&amp;rdquo; 的形式描述出来时，就形成了一种范式化的领域建模规约。
如下是使用 Ginkgo 测试框架搭建的测试用例，描述的业务场景是根据书本页数（Book.Pages）对书进行分类，小于 300 页应为短篇，大于 300 页应为小说：
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 var _ = Describe(&amp;#34;Books&amp;#34;, func() { var foxInSocks, lesMis *books.Book BeforeEach(func() { lesMis = &amp;amp;books.Book{ Title: &amp;#34;Les Miserables&amp;#34;, Author: &amp;#34;Victor Hugo&amp;#34;, Pages: 2783, } foxInSocks = &amp;amp;books.</description></item><item><title>管理项目中的 Go 二进制工具</title><link>https://huanggze.top/posts/manage-go-tools/</link><pubDate>Thu, 24 Feb 2022 19:21:30 +0800</pubDate><guid>https://huanggze.top/posts/manage-go-tools/</guid><description>有时候，在我们的项目中需要使用一些 Go 命令行工具，比如 ginkgo 测试工具、CI 工具、代码自动生成工具 client-gen 等。因此，需要确保 Go 命令行工具在 CI 服务器上等不同环境版本一致。
解决办法是把依赖的工具加到 go module 中。创建 tools.go，在文件中引用依赖工具包：
1 2 3 4 5 6 7 // +build tools package main import ( _ &amp;#34;github.com/onsi/ginkgo/v2/ginkgo&amp;#34; ) 编译标签：
注意 tools.go 中我们使用了编译标签（build tag）1// +build tools，作用是 go build 时会过滤掉带有标签的 go 文件，除非使用 go build -tags 指定标签编译。因为 tools.go 带有标签，不会被编译，仅用于指定 go 二进制安装版本。
这样我们就可以使用 go install github.com/onsi/ginkgo/v2/ginkgo 安装到指定版本的 ginkgo 工具。go install 会把安装到目录下。
go install 使用：
只有在包含 go module 的项目中使用 go install 不需要指定版本。如果在项目外运行 go install 会报错：go install: version is required when current directory is not in a module。此时需要 go install github.</description></item><item><title>Istio Egress 网关实践</title><link>https://huanggze.top/posts/istio-egress/</link><pubDate>Fri, 18 Feb 2022 21:34:49 +0800</pubDate><guid>https://huanggze.top/posts/istio-egress/</guid><description>本文介绍如何配置 Istio 使服务网格内的服务通过 Egress 网关访问外部服务。实验架构图如下，服务网格运行在 K8s 集群上，外部服务单独部署在集群外的一台虚拟机上。
环境准备 使用阿里云部署一个托管单节点 K8s 集群，再在同一 VPC 下创建一个 ECS 实例（假设 IP 地址为 192.168.0.78）用于部署外部服务。在 K8s 集群上安装 Istio，注意：1. 安装时同时开启 Egress Gateway；2.设置只允许访问注册进服务网格的服务；3. 开启 Envoy 访问日志（别忘了！）1：
1 2 3 4 5 istioctl install \ --set components.egressGateways[0].name=istio-egressgateway \ --set components.egressGateways[0].enabled=true \ --set meshConfig.outboundTrafficPolicy.mode=REGISTRY_ONLY \ --set meshConfig.accessLogFile=/dev/stdout 在集群外的这个虚拟机（192.168.0.78）上开启 HTTP 服务，监听端口为 1234：
1 python -m http.server 1234 在 K8s 集群上部署一个开启 istio 代理的 Pod 用于作为测试客户端来访问：
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 kubectl label namespace default istio-injection=enabled kubectl apply -f - &amp;lt;&amp;lt;EOF apiVersion: apps/v1 kind: Deployment metadata: name: sleep spec: selector: matchLabels: app: sleep template: metadata: labels: app: sleep spec: containers: - name: sleep image: curlimages/curl command: [&amp;#34;/bin/sleep&amp;#34;, &amp;#34;3650d&amp;#34;] imagePullPolicy: IfNotPresent EOF 尝试从 sleep Pod 中测试访问 192.</description></item><item><title>Istio 多集群部署（四）：多网络主从架构</title><link>https://huanggze.top/posts/istio-multicluster-deployment-part4/</link><pubDate>Mon, 31 Jan 2022 11:16:04 +0800</pubDate><guid>https://huanggze.top/posts/istio-multicluster-deployment-part4/</guid><description>多网络主从架构 多网络、主从架构安装可参考官方文档 Install Primary-Remote on different networks 。主从架构中，主集群的网关不仅暴露网格内的服务，还有暴露 Istio 控制平面的作用。架构示意图如下：
环境准备 本文使用阿里云托管 K8s 服务，在两个不同 VPC 下，分别部署一个 K8s 集群（命名 cluster1 和 cluster2，本示例部署的是单 worker node 集群），模拟多网络、多主集群。注意，创建集群时，「集群配置 &amp;gt; API Server 访问」一栏中，勾选「使用 EIP 暴露 API Server」。这使的 kube-apiserver 可以公网访问，用于公网连接的 kubeconfig 在集群页「连接信息 &amp;gt; 公网访问」中拿到。
在两个集群上，下载安装 istioctl（1.12.2 版本）：
1 2 3 curl -O https://ghproxy.com/https://github.com/istio/istio/releases/download/1.12.2/istioctl-1.12.2-linux-amd64.tar.gz tar zxvf istioctl-1.12.2-linux-amd64.tar.gz mv istioctl /usr/local/bin/ 最后，为了启用 kubectl 命令，拷贝一份内网访问 kubeconfig（通过阿里云控制台，容器服务 &amp;gt; 集群列表 &amp;gt; 集群信息 &amp;gt; 连接信息，获取）到各自集群 worker node 机器上的 .kube/ 目录下，文件命名为 config。
部署 Istio 控制平面 Step 1.</description></item><item><title>Istio 支持虚拟机集成实践</title><link>https://huanggze.top/posts/istio-virtual-machines/</link><pubDate>Sat, 29 Jan 2022 14:52:28 +0800</pubDate><guid>https://huanggze.top/posts/istio-virtual-machines/</guid><description>Istio 支持 K8s 集群外的虚拟机及虚拟机运行的应用加入 Istio 服务网格。这允许老的应用以及不适合容器化部署的应用也能使用 Istio 服务网格。 在单网络下，Gateway 负责虚拟机访问允许在 K8s 上的 Istio 控制平面：
多网络下，Gateway 负责同时暴露 Istio 控制平面和服务网格内的服务：
虚拟机集成 Step 1. 环境准备 本文使用阿里云，在同一 VPC 下，部署一个 K8s 托管集群以及一个 ECS 实例，模拟单网络下集成虚拟机。在 K8s 节点机器上设置以下环境变量用于后续操作，并创建工作目录 mkdir -p &amp;quot;${WORK_DIR}&amp;quot;。
1 2 3 4 5 6 7 8 9 10 11 VM_APP=&amp;#34;demo&amp;#34; VM_NAMESPACE=&amp;#34;vm&amp;#34; WORK_DIR=&amp;#34;${HOME}/vmintegration&amp;#34; SERVICE_ACCOUNT=&amp;#34;vm&amp;#34; # 多网络下，K8s 集群和虚拟机所在网络分别命名，如： # CLUSTER_NETWORK=&amp;#34;kube-network&amp;#34; # VM_NETWORK=&amp;#34;vm-network&amp;#34; # 而 CLUSTER 值赋为 cluster1 CLUSTER_NETWORK=&amp;#34;&amp;#34; VM_NETWORK=&amp;#34;&amp;#34; CLUSTER=&amp;#34;Kubernetes&amp;#34; Step 2. 安装 Istio 控制平面 在 K8s 集群上安装 Istio（如果集群已安装则无须安装，但仍要暴露 Istio 控制平面）：</description></item><item><title>Istio 多集群部署（三）：多网络多主架构</title><link>https://huanggze.top/posts/istio-multicluster-deployment-part3/</link><pubDate>Sat, 29 Jan 2022 13:32:02 +0800</pubDate><guid>https://huanggze.top/posts/istio-multicluster-deployment-part3/</guid><description>多网络、多主架构 多网络指多集群间网络隔离，Pod 与 Pod 不互通。因此 API Server 需要暴露公网，且需要分别配置网关，使 Pod 与 Pod 通过网关通信。架构示意图如下，部署教程参考官方文档 Install Multi-Primary on different networks：
环境准备 本文使用阿里云托管 K8s 服务，在两个不同 VPC 下，分别部署一个 K8s 集群（命名 cluster1 和 cluster2，本示例部署的是单 worker node 集群），模拟多网络、多主集群。注意，创建集群时，「集群配置 &amp;gt; API Server 访问」一栏中，勾选「使用 EIP 暴露 API Server」。这使的 kube-apiserver 可以公网访问，用于公网连接的 kubeconfig 在集群页「连接信息 &amp;gt; 公网访问」中拿到。
在两个集群上，下载安装 istioctl（1.12.2 版本）：
1 2 3 curl -O https://ghproxy.com/https://github.com/istio/istio/releases/download/1.12.2/istioctl-1.12.2-linux-amd64.tar.gz tar zxvf istioctl-1.12.2-linux-amd64.tar.gz mv istioctl /usr/local/bin/ 最后，为了启用 kubectl 命令，拷贝一份内网访问 kubeconfig（通过阿里云控制台，容器服务 &amp;gt; 集群列表 &amp;gt; 集群信息 &amp;gt; 连接信息，获取）到各自集群 worker node 机器上的 .</description></item><item><title>Istio 多集群部署（二）：单一网络主从架构</title><link>https://huanggze.top/posts/istio-multicluster-deployment-part2/</link><pubDate>Fri, 28 Jan 2022 23:24:39 +0800</pubDate><guid>https://huanggze.top/posts/istio-multicluster-deployment-part2/</guid><description>主从架构 主从架构指主集群安装 Istio 控制平面，从集群（remote cluster）连接主集群 Istio 控制平面。在主从架构中，从集群需要通过专门的 gateway 访问主集群上的 Istio 控制平面。简言之，如果集群内部署有 Istio 控制平面，该集群内的工作负载实例访问集群内的控制平面（主集群模式），否则访问外部 Istio 控制平面（从集群模式）。下图展示的是单网格、单网络、主从架构部署：
部署测试 参考官方文档部署 Install Primary-Remote 即可，前期工作与上一篇类似。有两点需要注意：
1. 配置 CA 中间证书 Configure Trust 必不可少，需要正确配置。否则会出现证书问题，istio-ingressgateway 无法启动，报错如下：
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 $ kubectl get po -n istio-system NAME READY STATUS RESTARTS AGE istio-ingressgateway-b68f578f6-p8j4p 0/1 Running 0 5m58s istiod-7cd5464766-hr8t4 1/1 Running 0 6m2s $ kubectl describe po -n istio-system istio-ingressgateway-b68f578f6-p8j4p Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 9m32s default-scheduler Successfully assigned istio-system/istio-ingressgateway-b68f578f6-p8j4p to cn-guangzhou.</description></item><item><title>Istio 多集群部署（一）：单一网络多主架构</title><link>https://huanggze.top/posts/istio-multicluster-deployment-part1/</link><pubDate>Fri, 28 Jan 2022 15:28:19 +0800</pubDate><guid>https://huanggze.top/posts/istio-multicluster-deployment-part1/</guid><description>单网格、单网络、多主架构 单网格、单网络、多主架构部署对应官方文档 Install Multi-Primary。单网格、单网络、多主架构部署指单个 Istio 服务网格（service mesh）运行在单个完全互联的网络上。网络内有多个集群，同时存在多个主集群（primary cluster）运行 Istio 控制平面。示例架构如下图：
单一网络模型，即所有工作负载实例（workload instances，指 pods）都可以直接相互访问、完全互联，而无需 Istio 网关。
注意：这里「可以直接相互访问」指的是 Pod 与 Pod 间互通（可互 ping），包括跨集群的 Pod 通信。不是指 Service 之间 Cluster IP 互相可 ping，Service 的 ClusterIP 不支持跨集群访问。ClusterIP 是虚拟 IP，没有对应实体，而跨集群 Pod IP 能互 ping 是因为路由表中存在对应网段的下一跳节点。
多主架构指多个集群下，存在多个单独部署的 Istio 控制平面。我们知道，Istio 控制平面通过向工作负载实例的 Envoy 代理下发服务端点信息实现流量管理。因此单网格下，Istio 控制平面需要拿到所有集群的服务端点信息。服务端点发现需要配置 Istio 控制平面使其能访问每个集群的 kube-apiserver1。
环境准备 本文使用阿里云托管 K8s 服务，在同一 VPC 下，部署两个集群（命名 cluster1 和 cluster2，本示例部署的是单 worker node 集群），模拟单网络、多集群。注意，在创建托管 K8s 界面里应设置 Pod CIDR 为不同网段，如 10.210.0.0/16 和 10.211.0.0/16。创建完后，检查跨集群 Pod 是否可以互相通信（互 ping Pod IP）。同一 VPC 下部署的集群 Pod 互通是因为 VPC 路由表存在对应的网段下一跳节点（通过阿里云控制台「专有网络 &amp;gt; 路由表」查看）。</description></item><item><title>K8s Operator 开发（三）：Cache 机制</title><link>https://huanggze.top/posts/k8s-operator-dev-part3/</link><pubDate>Wed, 19 Jan 2022 17:45:22 +0800</pubDate><guid>https://huanggze.top/posts/k8s-operator-dev-part3/</guid><description>Cache 上一篇谈到 Kubebuilder 使用的是读写分离的客户端。读客户端采用了 Cache 设计。通过深入追踪 main.go 的 ctrl.NewManager 代码，我们可以看到 Kubebuilder 使用 cotroller-runtime 包构造并持有了一个 Cache 接口实例。Cache 接口包含两个组件：Reader 和 Informers。而 Informers 本身内嵌了 FieldIndexer 接口。
1 2 3 4 5 6 7 8 9 10 // Cache knows how to load Kubernetes objects, fetch informers to request // to receive events for Kubernetes objects (at a low-level), // and add indices to fields on the objects stored in the cache. type Cache interface { // Cache acts as a client to objects stored in the cache.</description></item><item><title>K8s Operator 开发（二）：K8s API</title><link>https://huanggze.top/posts/k8s-operator-dev-part2/</link><pubDate>Mon, 17 Jan 2022 21:33:05 +0800</pubDate><guid>https://huanggze.top/posts/k8s-operator-dev-part2/</guid><description>K8s API Operator 通过与 kube-apiserver 通信访问 K8s 资源和自定义资源。kube-apiserver 是 Kubernetes 集群的核心组件和入口，暴露 RESTful HTTP API 接口，支持标准的 POST，GET，UPDATE，DELETE，PATCH 方法以及额外支持 WATCH1 和 LIST 操作。以下是的例子，K8s API 文档可以参考 Kubernetes API Reference Docs2。
1 2 3 4 5 6 7 8 9 10 11 # 创建 Deployment POST /apis/apps/v1/namespaces/{namespace}/deployments # 修改 Deployment PATCH /apis/apps/v1/namespaces/{namespace}/deployments/{name} # LIST 命名空间下所有 Deployment GET /apis/apps/v1/namespaces/default/deployments # 监听 Nginx Deployment 的增删改事件通知 GET /apis/apps/v1/watch/namespaces/{namespace}/deployments?watch=true&amp;amp;fieldSelector=metadata.name=nginx 大多数 Kubernetes 资源访问的 API 路径是 /apis/{group}/{version}/namespaces/{namespace}/{resource}/{resourceName}，比如 Deployment、Ingress 包括上一篇中的自定义资源 RebootPolicy，都有 API Group 和 Version。除了 K8s 早期的资源 Service、Node、Pod 由于历史原因，没有 API Group 只有版本（一般会称作 core API Group），路径是以 /api/v1 开头，如下图第二、三分支所示。这些常见的 K8s 数据类型都是版本化（versioned）、结构化（structured）。版本化指同一资源不同版本之间支持字段有差异；结构化指资源对象有对应的 Go 结构体，保证序列化和反序列化。URI 上使用 group、version、resource 来定位一个资源的形式称为 GVR，这和我们在 YAML 文件中使用的 apiVersion、kind 字段（又称 GVK 组合）是对应的。下一节我们会辨析 GVR、GVK 以及 Go Type 三者关系。</description></item><item><title>K8s Operator 开发（一）：概述</title><link>https://huanggze.top/posts/k8s-operator-dev-part1/</link><pubDate>Sat, 15 Jan 2022 21:20:28 +0800</pubDate><guid>https://huanggze.top/posts/k8s-operator-dev-part1/</guid><description>Operator 控制器 K8s 定义了很多抽象内部资源来描述不同工作负载类型，比如 Deployment 用于无状态应用1部署、StatefulSet 用于有状态应用、CronJob 适用于运行定时任务。
Deployment 和 StatefulSet 的区别2：
Deployment 创建的 Pod 之间没有顺序，服务通过 Service 的 Service IP 暴露。Deployment 也可以使用持久化存储卷实现有状态应用部署，但有使用限制。Deployment 只支持通过 .spec.template.spec.volumes.persistentVolumeClaim 引用一个 PVC（提前创建）。如果该 PVC 访问模式支持且设置为 RWO，Deployment 副本数量必须为 1（单 Pod）；否则，使用 RWX 模式，多个 Pod 共享存储。 StatefulSet：每个 Pod 有自己的存储，通过 .spec.volumeClaimTemplates 为每个 Pod 创建一个独立的 PV 保存其数据和状态。即使删除 StatefulSet 或 Pod 宕机，创建的 PVC 仍保留其数据并可以在 Pod 恢复后重新恢复绑定。StatefulSet 和无头服务配合使用（.spec.clusterIP=None），无头服务不做负载均衡，返回所有关联 Pod 的 IP 地址列表。 这些 K8s 内部资源的状态由对应资源的控制器来维护，比如 Deployment 对应 Deployment Controller。K8s 控制组件 kube-controller-manager 包含了所有内部资源控制器。控制器本质上是一个控制回路（control loop）无限循环进程，Watch 资源状态，并做出相应调整，调协当前状态（status）至期望状态（spec），如：滚动更新，恢复宕机的 Pod。对于运行在 Pod 中的程序，如下图3中的 DB 和 Web 程序，他们本身并无感知自身运行在 K8s 环境中。应用运维由 K8s 控制器来完成。</description></item><item><title>About</title><link>https://huanggze.top/about/</link><pubDate>Fri, 16 Jul 2021 11:24:06 +0800</pubDate><guid>https://huanggze.top/about/</guid><description/></item></channel></rss>