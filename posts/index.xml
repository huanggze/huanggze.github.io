<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on Hi Folks</title><link>https://huanggze.github.io/posts/</link><description>Recent content in Posts on Hi Folks</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>© huanggze 2021</copyright><lastBuildDate>Fri, 25 Feb 2022 21:32:07 +0800</lastBuildDate><atom:link href="https://huanggze.github.io/posts/index.xml" rel="self" type="application/rss+xml"/><item><title>Ginkgo 测试框架介绍</title><link>https://huanggze.github.io/posts/intro-ginkgo/</link><pubDate>Fri, 25 Feb 2022 21:32:07 +0800</pubDate><guid>https://huanggze.github.io/posts/intro-ginkgo/</guid><description>除了 Go testing 包提供的测试框架，还可以使用 Ginkgo 测试框架。Ginkgo 是一个行为驱动开发（Behavior Driven Development，BDD）测试框架。BDD 是一种敏捷开发技术，建立在测试驱动开发（Test Driven Development，TDD）基础之上，强调使用 DSL（Domain Specific Language，领域特定语言）描述用户行为、定义业务需求，是需求分析人员、开发人员与测试人员进行沟通的有效方法1。行为驱动开发的核心在于&amp;quot;行为&amp;quot;。当业务需求被划分为不同的业务场景，并以 &amp;ldquo;Given-When-Then&amp;rdquo; 的形式描述出来时，就形成了一种范式化的领域建模规约。
如下是使用 Ginkgo 测试框架搭建的测试用例，描述的业务场景是根据书本页数（Book.Pages）对书进行分类，小于 300 页应为短篇，大于 300 页应为小说：
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 var _ = Describe(&amp;#34;Books&amp;#34;, func() { var foxInSocks, lesMis *books.Book BeforeEach(func() { lesMis = &amp;amp;books.</description></item><item><title>管理 Go 命令行工具</title><link>https://huanggze.github.io/posts/manage-go-tools/</link><pubDate>Thu, 24 Feb 2022 19:21:30 +0800</pubDate><guid>https://huanggze.github.io/posts/manage-go-tools/</guid><description>有时候，在我们的项目中需要使用一些 Go 命令行工具，比如 ginkgo 测试工具、CI 工具、代码自动生成工具 client-gen 等。因此，需要确保 Go 命令行工具在 CI 服务器上等不同环境版本一致。
解决办法是把依赖的工具加到 go module 中。创建 tools.go，在文件中引用依赖工具包：
1 2 3 4 5 6 7 // +build tools package main import ( _ &amp;#34;github.com/onsi/ginkgo/v2/ginkgo&amp;#34; ) 编译标签：
注意 tools.go 中我们使用了编译标签（build tag）1// +build tools，作用是 go build 时会过滤掉带有标签的 go 文件，除非使用 go build -tags 指定标签编译。因为 tools.go 带有标签，不会被编译，仅用于指定 go 二进制安装版本。
这样我们就可以使用 go install github.com/onsi/ginkgo/v2/ginkgo 安装到指定版本的 ginkgo 工具。go install 会把安装到目录下。
go install 使用：</description></item><item><title>Istio Egress 网关实践</title><link>https://huanggze.github.io/posts/istio-egress/</link><pubDate>Fri, 18 Feb 2022 21:34:49 +0800</pubDate><guid>https://huanggze.github.io/posts/istio-egress/</guid><description>本文介绍如何配置 Istio 使服务网格内的服务通过 Egress 网关访问外部服务。实验架构图如下，服务网格运行在 K8s 集群上，外部服务单独部署在集群外的一台虚拟机上。
环境准备 使用阿里云部署一个托管单节点 K8s 集群，再在同一 VPC 下创建一个 ECS 实例（假设 IP 地址为 192.168.0.78）用于部署外部服务。在 K8s 集群上安装 Istio，注意：1. 安装时同时开启 Egress Gateway；2.设置只允许访问注册进服务网格的服务；3. 开启 Envoy 访问日志（别忘了！）1：
1 2 3 4 5 istioctl install \ --set components.egressGateways[0].name=istio-egressgateway \ --set components.egressGateways[0].enabled=true \ --set meshConfig.outboundTrafficPolicy.mode=REGISTRY_ONLY \ --set meshConfig.accessLogFile=/dev/stdout 在集群外的这个虚拟机（192.168.0.78）上开启 HTTP 服务，监听端口为 1234：
1 python -m http.server 1234 在 K8s 集群上部署一个开启 istio 代理的 Pod 用于作为测试客户端来访问：</description></item><item><title>Istio 多集群部署（四）：多网络主从架构</title><link>https://huanggze.github.io/posts/istio-multicluster-deployment-part4/</link><pubDate>Mon, 31 Jan 2022 11:16:04 +0800</pubDate><guid>https://huanggze.github.io/posts/istio-multicluster-deployment-part4/</guid><description>多网络主从架构 多网络、主从架构安装可参考官方文档 Install Primary-Remote on different networks 。主从架构中，主集群的网关不仅暴露网格内的服务，还有暴露 Istio 控制平面的作用。架构示意图如下：
环境准备 本文使用阿里云托管 K8s 服务，在两个不同 VPC 下，分别部署一个 K8s 集群（命名 cluster1 和 cluster2，本示例部署的是单 worker node 集群），模拟多网络、多主集群。注意，创建集群时，「集群配置 &amp;gt; API Server 访问」一栏中，勾选「使用 EIP 暴露 API Server」。这使的 kube-apiserver 可以公网访问，用于公网连接的 kubeconfig 在集群页「连接信息 &amp;gt; 公网访问」中拿到。
在两个集群上，下载安装 istioctl（1.12.2 版本）：
1 2 3 curl -O https://ghproxy.com/https://github.com/istio/istio/releases/download/1.12.2/istioctl-1.12.2-linux-amd64.tar.gz tar zxvf istioctl-1.12.2-linux-amd64.tar.gz mv istioctl /usr/local/bin/ 最后，为了启用 kubectl 命令，拷贝一份内网访问 kubeconfig（通过阿里云控制台，容器服务 &amp;gt; 集群列表 &amp;gt; 集群信息 &amp;gt; 连接信息，获取）到各自集群 worker node 机器上的 .kube/ 目录下，文件命名为 config。</description></item><item><title>Istio 支持虚拟机集成实践</title><link>https://huanggze.github.io/posts/istio-virtual-machines/</link><pubDate>Sat, 29 Jan 2022 14:52:28 +0800</pubDate><guid>https://huanggze.github.io/posts/istio-virtual-machines/</guid><description>Istio 支持 K8s 集群外的虚拟机及虚拟机运行的应用加入 Istio 服务网格。这允许老的应用以及不适合容器化部署的应用也能使用 Istio 服务网格。 在单网络下，Gateway 负责虚拟机访问允许在 K8s 上的 Istio 控制平面：
多网络下，Gateway 负责同时暴露 Istio 控制平面和服务网格内的服务：
虚拟机集成 Step 1. 环境准备 本文使用阿里云，在同一 VPC 下，部署一个 K8s 托管集群以及一个 ECS 实例，模拟单网络下集成虚拟机。在 K8s 节点机器上设置以下环境变量用于后续操作，并创建工作目录 mkdir -p &amp;quot;${WORK_DIR}&amp;quot;。
1 2 3 4 5 6 7 8 9 10 11 VM_APP=&amp;#34;demo&amp;#34; VM_NAMESPACE=&amp;#34;vm&amp;#34; WORK_DIR=&amp;#34;${HOME}/vmintegration&amp;#34; SERVICE_ACCOUNT=&amp;#34;vm&amp;#34; # 多网络下，K8s 集群和虚拟机所在网络分别命名，如： # CLUSTER_NETWORK=&amp;#34;kube-network&amp;#34; # VM_NETWORK=&amp;#34;vm-network&amp;#34; # 而 CLUSTER 值赋为 cluster1 CLUSTER_NETWORK=&amp;#34;&amp;#34; VM_NETWORK=&amp;#34;&amp;#34; CLUSTER=&amp;#34;Kubernetes&amp;#34; Step 2. 安装 Istio 控制平面 在 K8s 集群上安装 Istio（如果集群已安装则无须安装，但仍要暴露 Istio 控制平面）：</description></item><item><title>Istio 多集群部署（三）：多网络多主架构</title><link>https://huanggze.github.io/posts/istio-multicluster-deployment-part3/</link><pubDate>Sat, 29 Jan 2022 13:32:02 +0800</pubDate><guid>https://huanggze.github.io/posts/istio-multicluster-deployment-part3/</guid><description>多网络、多主架构 多网络指多集群间网络隔离，Pod 与 Pod 不互通。因此 API Server 需要暴露公网，且需要分别配置网关，使 Pod 与 Pod 通过网关通信。架构示意图如下，部署教程参考官方文档 Install Multi-Primary on different networks：
环境准备 本文使用阿里云托管 K8s 服务，在两个不同 VPC 下，分别部署一个 K8s 集群（命名 cluster1 和 cluster2，本示例部署的是单 worker node 集群），模拟多网络、多主集群。注意，创建集群时，「集群配置 &amp;gt; API Server 访问」一栏中，勾选「使用 EIP 暴露 API Server」。这使的 kube-apiserver 可以公网访问，用于公网连接的 kubeconfig 在集群页「连接信息 &amp;gt; 公网访问」中拿到。
在两个集群上，下载安装 istioctl（1.12.2 版本）：
1 2 3 curl -O https://ghproxy.com/https://github.com/istio/istio/releases/download/1.12.2/istioctl-1.12.2-linux-amd64.tar.gz tar zxvf istioctl-1.12.2-linux-amd64.tar.gz mv istioctl /usr/local/bin/ 最后，为了启用 kubectl 命令，拷贝一份内网访问 kubeconfig（通过阿里云控制台，容器服务 &amp;gt; 集群列表 &amp;gt; 集群信息 &amp;gt; 连接信息，获取）到各自集群 worker node 机器上的 .</description></item><item><title>Istio 多集群部署（二）：单一网络主从架构</title><link>https://huanggze.github.io/posts/istio-multicluster-deployment-part2/</link><pubDate>Fri, 28 Jan 2022 23:24:39 +0800</pubDate><guid>https://huanggze.github.io/posts/istio-multicluster-deployment-part2/</guid><description>主从架构 主从架构指主集群安装 Istio 控制平面，从集群（remote cluster）连接主集群 Istio 控制平面。在主从架构中，从集群需要通过专门的 gateway 访问主集群上的 Istio 控制平面。简言之，如果集群内部署有 Istio 控制平面，该集群内的工作负载实例访问集群内的控制平面（主集群模式），否则访问外部 Istio 控制平面（从集群模式）。下图展示的是单网格、单网络、主从架构部署：
部署测试 参考官方文档部署 Install Primary-Remote 即可，前期工作与上一篇类似。有两点需要注意：
1. 配置 CA 中间证书 Configure Trust 必不可少，需要正确配置。否则会出现证书问题，istio-ingressgateway 无法启动，报错如下：
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 $ kubectl get po -n istio-system NAME READY STATUS RESTARTS AGE istio-ingressgateway-b68f578f6-p8j4p 0/1 Running 0 5m58s istiod-7cd5464766-hr8t4 1/1 Running 0 6m2s $ kubectl describe po -n istio-system istio-ingressgateway-b68f578f6-p8j4p Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 9m32s default-scheduler Successfully assigned istio-system/istio-ingressgateway-b68f578f6-p8j4p to cn-guangzhou.</description></item><item><title>Istio 多集群部署（一）：单一网络多主架构</title><link>https://huanggze.github.io/posts/istio-multicluster-deployment-part1/</link><pubDate>Fri, 28 Jan 2022 15:28:19 +0800</pubDate><guid>https://huanggze.github.io/posts/istio-multicluster-deployment-part1/</guid><description>单网格、单网络、多主架构 单网格、单网络、多主架构部署对应官方文档 Install Multi-Primary。单网格、单网络、多主架构部署指单个 Istio 服务网格（service mesh）运行在单个完全互联的网络上。网络内有多个集群，同时存在多个主集群（primary cluster）运行 Istio 控制平面。示例架构如下图：
单一网络模型，即所有工作负载实例（workload instances，指 pods）都可以直接相互访问、完全互联，而无需 Istio 网关。
注意：这里「可以直接相互访问」指的是 Pod 与 Pod 间互通（可互 ping），包括跨集群的 Pod 通信。不是指 Service 之间 Cluster IP 互相可 ping，Service 的 ClusterIP 不支持跨集群访问。ClusterIP 是虚拟 IP，没有对应实体，而跨集群 Pod IP 能互 ping 是因为路由表中存在对应网段的下一跳节点。
多主架构指多个集群下，存在多个单独部署的 Istio 控制平面。我们知道，Istio 控制平面通过向工作负载实例的 Envoy 代理下发服务端点信息实现流量管理。因此单网格下，Istio 控制平面需要拿到所有集群的服务端点信息。服务端点发现需要配置 Istio 控制平面使其能访问每个集群的 kube-apiserver1。
环境准备 本文使用阿里云托管 K8s 服务，在同一 VPC 下，部署两个集群（命名 cluster1 和 cluster2，本示例部署的是单 worker node 集群），模拟单网络、多集群。注意，在创建托管 K8s 界面里应设置 Pod CIDR 为不同网段，如 10.210.0.0/16 和 10.</description></item><item><title>K8s Operator 开发（三）：Cache 机制</title><link>https://huanggze.github.io/posts/k8s-operator-dev-part3/</link><pubDate>Wed, 19 Jan 2022 17:45:22 +0800</pubDate><guid>https://huanggze.github.io/posts/k8s-operator-dev-part3/</guid><description>Cache 上一篇谈到 Kubebuilder 使用的是读写分离的客户端。读客户端采用了 Cache 设计。通过深入追踪 main.go 的 ctrl.NewManager 代码，我们可以看到 Kubebuilder 使用 cotroller-runtime 包构造并持有了一个 Cache 接口实例。Cache 接口包含两个组件：Reader 和 Informers。而 Informers 本身内嵌了 FieldIndexer 接口。
1 2 3 4 5 6 7 8 9 10 // Cache knows how to load Kubernetes objects, fetch informers to request // to receive events for Kubernetes objects (at a low-level), // and add indices to fields on the objects stored in the cache. type Cache interface { // Cache acts as a client to objects stored in the cache.</description></item><item><title>K8s Operator 开发（二）：K8s API</title><link>https://huanggze.github.io/posts/k8s-operator-dev-part2/</link><pubDate>Mon, 17 Jan 2022 21:33:05 +0800</pubDate><guid>https://huanggze.github.io/posts/k8s-operator-dev-part2/</guid><description>K8s API Operator 通过与 kube-apiserver 通信访问 K8s 资源和自定义资源。kube-apiserver 是 Kubernetes 集群的核心组件和入口，暴露 RESTful HTTP API 接口，支持标准的 POST，GET，UPDATE，DELETE，PATCH 方法以及额外支持 WATCH1 和 LIST 操作。以下是的例子，K8s API 文档可以参考 Kubernetes API Reference Docs2。
1 2 3 4 5 6 7 8 9 10 11 # 创建 Deployment POST /apis/apps/v1/namespaces/{namespace}/deployments # 修改 Deployment PATCH /apis/apps/v1/namespaces/{namespace}/deployments/{name} # LIST 命名空间下所有 Deployment GET /apis/apps/v1/namespaces/default/deployments # 监听 Nginx Deployment 的增删改事件通知 GET /apis/apps/v1/watch/namespaces/{namespace}/deployments?watch=true&amp;amp;fieldSelector=metadata.name=nginx 大多数 Kubernetes 资源访问的 API 路径是 /apis/{group}/{version}/namespaces/{namespace}/{resource}/{resourceName}，比如 Deployment、Ingress 包括上一篇中的自定义资源 RebootPolicy，都有 API Group 和 Version。除了 K8s 早期的资源 Service、Node、Pod 由于历史原因，没有 API Group 只有版本（一般会称作 core API Group），路径是以 /api/v1 开头，如下图第二、三分支所示。这些常见的 K8s 数据类型都是版本化（versioned）、结构化（structured）。版本化指同一资源不同版本之间支持字段有差异；结构化指资源对象有对应的 Go 结构体，保证序列化和反序列化。URI 上使用 group、version、resource 来定位一个资源的形式称为 GVR，这和我们在 YAML 文件中使用的 apiVersion、kind 字段（又称 GVK 组合）是对应的。下一节我们会辨析 GVR、GVK 以及 Go Type 三者关系。</description></item><item><title>K8s Operator 开发（一）：概述</title><link>https://huanggze.github.io/posts/k8s-operator-dev-part1/</link><pubDate>Sat, 15 Jan 2022 21:20:28 +0800</pubDate><guid>https://huanggze.github.io/posts/k8s-operator-dev-part1/</guid><description>Operator 控制器 K8s 定义了很多抽象内部资源来描述不同工作负载类型，比如 Deployment 用于无状态应用1部署、StatefulSet 用于有状态应用、CronJob 适用于运行定时任务。
Deployment 和 StatefulSet 的区别2：
Deployment 创建的 Pod 之间没有顺序，服务通过 Service 的 Service IP 暴露。Deployment 也可以使用持久化存储卷实现有状态应用部署，但有使用限制。Deployment 只支持通过 .spec.template.spec.volumes.persistentVolumeClaim 引用一个 PVC（提前创建）。如果该 PVC 访问模式支持且设置为 RWO，Deployment 副本数量必须为 1（单 Pod）；否则，使用 RWX 模式，多个 Pod 共享存储。 StatefulSet：每个 Pod 有自己的存储，通过 .spec.volumeClaimTemplates 为每个 Pod 创建一个独立的 PV 保存其数据和状态。即使删除 StatefulSet 或 Pod 宕机，创建的 PVC 仍保留其数据并可以在 Pod 恢复后重新恢复绑定。StatefulSet 和无头服务配合使用（.spec.clusterIP=None），无头服务不做负载均衡，返回所有关联 Pod 的 IP 地址列表。 这些 K8s 内部资源的状态由对应资源的控制器来维护，比如 Deployment 对应 Deployment Controller。K8s 控制组件 kube-controller-manager 包含了所有内部资源控制器。控制器本质上是一个控制回路（control loop）无限循环进程，Watch 资源状态，并做出相应调整，调协当前状态（status）至期望状态（spec），如：滚动更新，恢复宕机的 Pod。对于运行在 Pod 中的程序，如下图3中的 DB 和 Web 程序，他们本身并无感知自身运行在 K8s 环境中。应用运维由 K8s 控制器来完成。</description></item></channel></rss>